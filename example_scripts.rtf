{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf200
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 ####normal function (part 1.5)###\
#n = normal(x, y)\
#initialMSE = MSE(W, b, x, y, reg)\
#finalMSE = MSE(n, b, x, y, reg)\
#print("Training MSE")\
#print(initialMSE)\
#print(finalMSE)\
#accuracyinitial = accuracy(W, b, x, y)\
#accuracyfinal = accuracy(n, b, x, y)\
#print("Training Accuracy")\
#print(accuracyinitial)\
#print(accuracyfinal)\
#################################\
\
###running gradient descent and plotting results#####\
initial = crossEntropyLoss(W, b, x, y, reg)\
W, b, Loss, LossValid, LossTest, iterations = grad_descent(W, b, x, y, LR, epochs, reg, error, validData, validTarget, testData, testTarget, lossType="CE")\
print('initial loss')\
print(initial)\
print("Training")\
print(Loss[0])\
print(Loss[4999])\
print("Valid")\
print(LossValid[0])\
print(LossValid[4999])\
print("Test")\
print(LossTest[0])\
print(LossTest[4999])\
\
plt.xlabel('Iterations')\
plt.ylabel('Loss')\
plt.title('0.005 LR Zero-Weight Decay Training Loss')\
plt.plot(iterations, Loss, 'r')\
plt.show()\
\
plt.xlabel('Iterations')\
plt.ylabel('Loss')\
plt.title('0.005 LR 0.1 Reg Valid Loss')\
plt.plot(iterations, LossValid, 'r')\
plt.show()\
\
plt.xlabel('Iterations')\
plt.ylabel('Loss')\
plt.title('0.005 LR 0.1 Reg Test Loss')\
plt.plot(iterations, LossTest, 'r')\
plt.show()\
##############################################\
\
\
\
GRAD DESCENT FUNCTION\
\
    #final accuracies\
    accuracyFinal = accuracy(W, b, trainingData, trainingLabels, lossType)\
    accuracyFinalV = accuracy(W, b, validData, validTarget, lossType)\
    accuracyFinalT = accuracy(W, b, testData, testTarget, lossType)\
    print(accuracies[0])\
    print(accuracies[1])\
    print(accuracyFinal)\
    #printing out initial and final accuracies for all data sets\
    #after running descent on training data\
    print("Training accuracy Test 0.1 Reg")\
    print(accuracyInitial)\
    print(accuracyFinal)\
    print("Valid")\
    print(accuracyInitial)\
    print(accuracyFinal)\
    print("Test")\
    print(accuracyInitial)\
    print(accuracyFinal)\
\
    #plotting accuracies\
    plt.xlabel('Iterations')\
    plt.ylabel('Accuracy')\
    plt.title('0.005 LR 0.1 Reg Training Accuracy')\
    plt.plot(iterationNum, accuracies, 'r')\
    plt.show()\
\
\
   VARIABLE STUFF\
        #Losses and accuracies stored for each iteration\
        #comment out what is not needed for graph\
        iterationNum[i] = i\
        MSEs[i] = MSE(W, b, trainingData, trainingLabels, reg)\
        MSEvalid[i] = MSE(W, b, validData, validTarget, reg)\
        MSEtest[i] = MSE(W, b, testData, testTarget, reg)\
        CEs[i] = crossEntropyLoss(W, b, trainingData, trainingLabels, reg)\
        CEvalid[i] = crossEntropyLoss(W, b, validData, validTarget, reg)\
        CEtest[i] = crossEntropyLoss(W, b, testData, testTarget, reg)\
        accuracies[i] = accuracy(W, b, x, y, lossType)\
\
\
    #all the loses of linear gradient descent\
    MSEs = np.zeros((iterations, 1))\
    MSEvalid = np.zeros((iterations, 1))\
    MSEtest = np.zeros((iterations, 1))\
    CEs = np.zeros((iterations, 1))\
    CEvalid = np.zeros((iterations, 1))\
    CEtest = np.zeros((iterations, 1))\
    #array of iterations\
    iterationNum = np.zeros((iterations, 1))\
    #accuracy classifications\
    accuracies = np.zeros((iterations, 1))\
    #initial accuracies\
    accuracyInitial = accuracy(W, b, trainingData, trainingLabels, lossType)\
    accuracyInitialV = accuracy(W, b, validData, validTarget, lossType)\
    accuracyInitialT = accuracy(W, b, testData, testTarget, lossType)\
}